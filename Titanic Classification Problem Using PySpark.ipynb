{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MrBC530gmg5"
   },
   "source": [
    "### **Problem** \n",
    "**Our goal is to create a predictive model that can answer the following question:**\n",
    "\n",
    "**What kind of people had a better chance of surviving?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhBfwzBgioTD"
   },
   "source": [
    "**Data about passengers:**\n",
    "*   Name\n",
    "*   Age\n",
    "*   Gender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIEH8iZqi-sk"
   },
   "source": [
    "## Install and Import Libraries\n",
    "Let's install PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oj1HhvIOY5Yz"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDp80mG9jmfU"
   },
   "source": [
    "## Build Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "ttzML9fpjE5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/05 07:47:23 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.153.130 instead (on interface ens33)\n",
      "21/10/05 07:47:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/05 07:47:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/05 07:47:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/05 07:47:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiqECDzLj1Mg"
   },
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn-hxNggkTqV"
   },
   "source": [
    "You have two datasets: \n",
    "* Train  \n",
    "* Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8-A8M7QmKDJ"
   },
   "source": [
    "Read two datasets: \n",
    "* Train\n",
    "* Test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj2ANTnWmSCq"
   },
   "source": [
    "Let's work with train dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5mWJR30lNs5"
   },
   "source": [
    "**Confirm if this is a dataframe or not:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tEYTePrzk9yl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvLJElPrlT4i"
   },
   "source": [
    "**Show 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jYwhqvV8lnO0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name                                               |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|1          |0       |3     |Braund, Mr. Owen Harris                            |male  |22.0|1    |0    |A/5 21171       |7.25   |null |S       |\n",
      "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
      "|3          |1       |3     |Heikkinen, Miss. Laina                             |female|26.0|0    |0    |STON/O2. 3101282|7.925  |null |S       |\n",
      "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
      "|5          |0       |3     |Allen, Mr. William Henry                           |male  |35.0|0    |0    |373450          |8.05   |null |S       |\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QIYVxRXlnnw"
   },
   "source": [
    "**Display schema for the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pcvERiICl1Ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmE3Wd80l1S6"
   },
   "source": [
    "**Statistical summary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cNY0SItol5Mo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+------------------------------------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|PassengerId      |Survived           |Pclass            |Name                                            |Sex   |Age               |SibSp             |Parch              |Ticket            |Fare             |Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+------------------------------------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|count  |891              |891                |891               |891                                             |891   |714               |891               |891                |891               |891              |204  |889     |\n",
      "|mean   |446.0            |0.3838383838383838 |2.308641975308642 |null                                            |null  |29.69911764705882 |0.5230078563411896|0.38159371492704824|260318.54916792738|32.2042079685746 |null |null    |\n",
      "|stddev |257.3538420152301|0.48659245426485753|0.8360712409770491|null                                            |null  |14.526497332334035|1.1027434322934315|0.8060572211299488 |471609.26868834975|49.69342859718089|null |null    |\n",
      "|min    |1                |0                  |1                 |\"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"|female|0.42              |0                 |0                  |110152            |0.0              |A10  |C       |\n",
      "|max    |891              |1                  |3                 |van Melkebeke, Mr. Philemon                     |male  |80.0              |8                 |6                  |WE/P 5735         |512.3292         |T    |S       |\n",
      "+-------+-----------------+-------------------+------------------+------------------------------------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiFaIEQTl70_"
   },
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSNPOnP8mw2Q"
   },
   "source": [
    "**Display count for the train dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zrtpG11Fl9HM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_6nnTfxm9_x"
   },
   "source": [
    "**Can you answer this question:** \n",
    "\n",
    "**How many people survived, and how many didn't survive?** \n",
    "\n",
    "**Please save data in a variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QDoqPwyomYxA"
   },
   "outputs": [],
   "source": [
    "survived_vs_not_survived = train_df.groupBy('Survived').agg((count('Survived')).alias('Count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8DUtZXPn46m"
   },
   "source": [
    "**Display your result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0XHAK8ceoCMU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:========================================>               (54 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|Count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "survived_vs_not_survived.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygsg7wQqor9a"
   },
   "source": [
    "**Can you display your answer in ratio form?(Hint: Use UDF.)**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PvzEwesgoQ3s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:============================================>           (60 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|Survived|             Ratio|\n",
      "+--------+------------------+\n",
      "|       1|0.3838383838383838|\n",
      "|       0|0.6161616161616161|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "survived_vs_not_survived.select(col('Survived'), (col('Count')/train_df.count()).alias('Ratio')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Aker_lp1h4"
   },
   "source": [
    "**Can you get the number of males and females?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XllkDlo3ongJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:====================================>                   (49 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|Count|\n",
      "+------+-----+\n",
      "|female|  314|\n",
      "|  male|  577|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Sex').agg((count('Sex')).alias('Count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHFaJ15zqtEV"
   },
   "source": [
    "**1. What is the average number of survivors of each gender?**\n",
    "\n",
    "**2. What is the number of survivors of each gender?**\n",
    "\n",
    "(Hint: Group by the \"sex\" column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NUikH7MUqdKq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+\n",
      "|   Sex|Survived|Number of Survivors|\n",
      "+------+--------+-------------------+\n",
      "|female|       1|                233|\n",
      "|  male|       1|                109|\n",
      "+------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy(['Sex', 'Survived']).agg((count('Survived')).alias('Number of Survivors')).filter(col('Survived') == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCEdYNdArtRN"
   },
   "source": [
    "**Create temporary view PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YjlK6HDUqsI5"
   },
   "outputs": [],
   "source": [
    "train_df.createOrReplaceTempView(\"train_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXNePifnshHr"
   },
   "source": [
    "**How many people survived, and how many didn't survive? By SQL:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0HxfPRTMslqk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|Count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql_statement_1 = '''select Survived, count(Survived) as Count\n",
    "                    from train_table\n",
    "                    group by Survived'''\n",
    "\n",
    "spark.sql(sql_statement_1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVCdY6EasFWV"
   },
   "source": [
    "**Can you display the number of survivors from each gender as a ratio?**\n",
    "\n",
    "(Hint: Group by \"sex\" column.)\n",
    "\n",
    "**Can you do this via SQL?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7xQc3pUUr3HF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|   Sex|Number_of_Survivors|\n",
      "+------+-------------------+\n",
      "|female|                233|\n",
      "|  male|                109|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_statement_2 = '''select Sex, count(Sex) as Number_of_Survivors\n",
    "                    from (select * from train_table where Survived == 1)\n",
    "                    group by Sex'''\n",
    "\n",
    "spark.sql(sql_statement_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6QXc5V8uu3Y"
   },
   "source": [
    "**Display a ratio for p-class:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Mscs2mDFdFsD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/05 07:48:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 73:===============================================>      (177 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|Pclass|              Ratio|\n",
      "+------+-------------------+\n",
      "|     1|0.24242424242424243|\n",
      "|     2|0.20650953984287318|\n",
      "|     3| 0.5510662177328844|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql_statement_3 = '''select Pclass, count(Pclass)/sum(count(*)) over() as Ratio\n",
    "                    from train_table\n",
    "                    group by Pclass\n",
    "                    order by Pclass asc'''\n",
    "\n",
    "spark.sql(sql_statement_3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX0klxwAvg6J"
   },
   "source": [
    "**Let's take a break and continue after this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ctM9t8atxJl"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CfanZTCt6Wk"
   },
   "source": [
    "**First and foremost, we must merge both the train and test datasets. (Hint: The union function can do this.)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8Nm8S1K0r4uY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+-------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name                                                   |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
      "+-----------+--------+------+-------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|1          |0       |3     |Braund, Mr. Owen Harris                                |male  |22.0|1    |0    |A/5 21171       |7.25   |null |S       |\n",
      "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)    |female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
      "|3          |1       |3     |Heikkinen, Miss. Laina                                 |female|26.0|0    |0    |STON/O2. 3101282|7.925  |null |S       |\n",
      "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)           |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
      "|5          |0       |3     |Allen, Mr. William Henry                               |male  |35.0|0    |0    |373450          |8.05   |null |S       |\n",
      "|6          |0       |3     |Moran, Mr. James                                       |male  |null|0    |0    |330877          |8.4583 |null |Q       |\n",
      "|7          |0       |1     |McCarthy, Mr. Timothy J                                |male  |54.0|0    |0    |17463           |51.8625|E46  |S       |\n",
      "|8          |0       |3     |Palsson, Master. Gosta Leonard                         |male  |2.0 |3    |1    |349909          |21.075 |null |S       |\n",
      "|9          |1       |3     |Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      |female|27.0|0    |2    |347742          |11.1333|null |S       |\n",
      "|10         |1       |2     |Nasser, Mrs. Nicholas (Adele Achem)                    |female|14.0|1    |0    |237736          |30.0708|null |C       |\n",
      "|11         |1       |3     |Sandstrom, Miss. Marguerite Rut                        |female|4.0 |1    |1    |PP 9549         |16.7   |G6   |S       |\n",
      "|12         |1       |1     |Bonnell, Miss. Elizabeth                               |female|58.0|0    |0    |113783          |26.55  |C103 |S       |\n",
      "|13         |0       |3     |Saundercock, Mr. William Henry                         |male  |20.0|0    |0    |A/5. 2151       |8.05   |null |S       |\n",
      "|14         |0       |3     |Andersson, Mr. Anders Johan                            |male  |39.0|1    |5    |347082          |31.275 |null |S       |\n",
      "|15         |0       |3     |Vestrom, Miss. Hulda Amanda Adolfina                   |female|14.0|0    |0    |350406          |7.8542 |null |S       |\n",
      "|16         |1       |2     |Hewlett, Mrs. (Mary D Kingcome)                        |female|55.0|0    |0    |248706          |16.0   |null |S       |\n",
      "|17         |0       |3     |Rice, Master. Eugene                                   |male  |2.0 |4    |1    |382652          |29.125 |null |Q       |\n",
      "|18         |1       |2     |Williams, Mr. Charles Eugene                           |male  |null|0    |0    |244373          |13.0   |null |S       |\n",
      "|19         |0       |3     |Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)|female|31.0|1    |0    |345763          |18.0   |null |S       |\n",
      "|20         |1       |3     |Masselmani, Mrs. Fatima                                |female|null|0    |0    |2649            |7.225  |null |C       |\n",
      "+-----------+--------+------+-------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_test_df = train_df.union(test_df)\n",
    "train_and_test_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI7AD8FLz3iO"
   },
   "source": [
    "**Display count:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "I4rd9e6nzzr5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1329"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVQlr9vDy7Y4"
   },
   "source": [
    "**Temporary view PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "s_WERAL8wvJa"
   },
   "outputs": [],
   "source": [
    "train_and_test_df.createOrReplaceTempView('train_test_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R4Miuy0z_uP"
   },
   "source": [
    "**Can you define the number of null values in each column?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0LMOalKBxhpD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|265|    0|    0|     0|   0| 1021|       3|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nulls = train_and_test_df.select([count(when(isnull(col), col)).alias(col) for col in train_and_test_df.columns])\n",
    "nulls.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBX8cJ000aqe"
   },
   "source": [
    "**Create Dataframe for null values**\n",
    "\n",
    "1. Column\n",
    "2. Number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ITmyUelNxjJM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+\n",
      "|Age|Cabin|Embarked|\n",
      "+---+-----+--------+\n",
      "|265| 1021|       3|\n",
      "+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df = nulls.select('Age', 'Cabin', 'Embarked')\n",
    "null_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuKrOi5a0-Ma"
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txa8NZIO1JaP"
   },
   "source": [
    "**Can you show me the name column from your temporary table?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "m7yXqJoJy35k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|name                                                   |\n",
      "+-------------------------------------------------------+\n",
      "|Braund, Mr. Owen Harris                                |\n",
      "|Cumings, Mrs. John Bradley (Florence Briggs Thayer)    |\n",
      "|Heikkinen, Miss. Laina                                 |\n",
      "|Futrelle, Mrs. Jacques Heath (Lily May Peel)           |\n",
      "|Allen, Mr. William Henry                               |\n",
      "|Moran, Mr. James                                       |\n",
      "|McCarthy, Mr. Timothy J                                |\n",
      "|Palsson, Master. Gosta Leonard                         |\n",
      "|Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      |\n",
      "|Nasser, Mrs. Nicholas (Adele Achem)                    |\n",
      "|Sandstrom, Miss. Marguerite Rut                        |\n",
      "|Bonnell, Miss. Elizabeth                               |\n",
      "|Saundercock, Mr. William Henry                         |\n",
      "|Andersson, Mr. Anders Johan                            |\n",
      "|Vestrom, Miss. Hulda Amanda Adolfina                   |\n",
      "|Hewlett, Mrs. (Mary D Kingcome)                        |\n",
      "|Rice, Master. Eugene                                   |\n",
      "|Williams, Mr. Charles Eugene                           |\n",
      "|Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)|\n",
      "|Masselmani, Mrs. Fatima                                |\n",
      "+-------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_statement_4 = '''select name\n",
    "                    from train_test_table'''\n",
    "\n",
    "spark.sql(sql_statement_4).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F0F9cTZ2Cuz"
   },
   "source": [
    "**Run this code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0kx6OcB-2BBT"
   },
   "outputs": [],
   "source": [
    "combined = train_and_test_df.withColumn('Title',regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
    "combined.createOrReplaceTempView('combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbZeUWS12r59"
   },
   "source": [
    "**Display the title and count \"Title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hGkFMtlp1FAI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   Title|Count|\n",
      "+--------+-----+\n",
      "|     Don|    1|\n",
      "|    Miss|  257|\n",
      "|Countess|    2|\n",
      "|     Col|    4|\n",
      "|     Rev|    9|\n",
      "|    Lady|    2|\n",
      "|  Master|   56|\n",
      "|     Mme|    1|\n",
      "|    Capt|    2|\n",
      "|      Mr|  786|\n",
      "|      Dr|   11|\n",
      "|     Mrs|  186|\n",
      "|     Sir|    2|\n",
      "|Jonkheer|    2|\n",
      "|    Mlle|    4|\n",
      "|   Major|    3|\n",
      "|      Ms|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.select('Title').count()\n",
    "\n",
    "\n",
    "sql_statement_5 = '''\n",
    "                    select Title, count(Title) as Count\n",
    "                    from combined\n",
    "                    group By Title\n",
    "                    '''\n",
    "titles_df = spark.sql(sql_statement_5)\n",
    "titles_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLBQDKYu4JOa"
   },
   "source": [
    "**We can see that Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, Countess, Ms, Sir, Lady, and Mme are really rare titles, so create Dictionary and set the value to \"rare\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Title='Don', Count=1),\n",
       " Row(Title='Miss', Count=257),\n",
       " Row(Title='Countess', Count=2),\n",
       " Row(Title='Col', Count=4),\n",
       " Row(Title='Rev', Count=9),\n",
       " Row(Title='Lady', Count=2),\n",
       " Row(Title='Master', Count=56),\n",
       " Row(Title='Mme', Count=1),\n",
       " Row(Title='Capt', Count=2),\n",
       " Row(Title='Mr', Count=786),\n",
       " Row(Title='Dr', Count=11),\n",
       " Row(Title='Mrs', Count=186),\n",
       " Row(Title='Sir', Count=2),\n",
       " Row(Title='Jonkheer', Count=2),\n",
       " Row(Title='Mlle', Count=4),\n",
       " Row(Title='Major', Count=3),\n",
       " Row(Title='Ms', Count=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = titles_df.collect()\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Don': 'rare',\n",
       " 'Miss': 'Miss',\n",
       " 'Countess': 'rare',\n",
       " 'Col': 'rare',\n",
       " 'Rev': 'rare',\n",
       " 'Lady': 'rare',\n",
       " 'Master': 'Master',\n",
       " 'Mme': 'rare',\n",
       " 'Capt': 'rare',\n",
       " 'Mr': 'Mr',\n",
       " 'Dr': 'rare',\n",
       " 'Mrs': 'Mrs',\n",
       " 'Sir': 'rare',\n",
       " 'Jonkheer': 'rare',\n",
       " 'Mlle': 'rare',\n",
       " 'Major': 'rare',\n",
       " 'Ms': 'rare'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rare_dict = {\n",
    "    titles[i][0] : 'rare'\n",
    "    if titles[i][1] <= 11 \n",
    "    else titles[i][0]\n",
    "    for i in range(len(titles))\n",
    "}\n",
    "\n",
    "\n",
    "rare_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wrE95Cv7Oqh"
   },
   "source": [
    "**Run the function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HdDbWuDl7Pf4"
   },
   "outputs": [],
   "source": [
    "def impute_title(title):\n",
    "    return rare_dict[title]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5EQVIhK7a9R"
   },
   "source": [
    "**Apply the function on \"Title\" column using UDF:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "rBAiIOn77XFa"
   },
   "outputs": [],
   "source": [
    "replaceUDF = udf(lambda z: impute_title(z))\n",
    "combined = combined.withColumn(\"Title\", replaceUDF(col(\"Title\")))\n",
    "combined.createOrReplaceTempView(\"combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn8ewllf7kiV"
   },
   "source": [
    "**Display \"Title\" from table and group by \"Title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "J9sjQb084GU6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Title |\n",
      "+------+\n",
      "|Mr    |\n",
      "|Mrs   |\n",
      "|Miss  |\n",
      "|Mrs   |\n",
      "|Mr    |\n",
      "|Mr    |\n",
      "|Mr    |\n",
      "|Master|\n",
      "|Mrs   |\n",
      "|Mrs   |\n",
      "|Miss  |\n",
      "|Miss  |\n",
      "|Mr    |\n",
      "|Mr    |\n",
      "|Miss  |\n",
      "|Mrs   |\n",
      "|Master|\n",
      "|Mr    |\n",
      "|Mrs   |\n",
      "|Mrs   |\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Title|\n",
      "+------+\n",
      "|  rare|\n",
      "|  Miss|\n",
      "|Master|\n",
      "|    Mr|\n",
      "|   Mrs|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.select('Title').show(truncate=False)\n",
    "\n",
    "\n",
    "sql_statement_6 = '''\n",
    "                    select Title\n",
    "                    from combined\n",
    "                    group By Title\n",
    "                    '''\n",
    "spark.sql(sql_statement_6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H45QNLj9vJp"
   },
   "source": [
    "## **Preprocessing Age**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwRAhumK-u__"
   },
   "source": [
    "**Based on the age mean, you will fill in the missing age values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "eXYSVzvl4z63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age = int(combined.select('Age').agg({'Age':'mean'}).collect()[0][0])\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLPivde8_GI-"
   },
   "source": [
    "**Fill missing age with age mean:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "lBgW8aFD90PA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| Age|\n",
      "+----+\n",
      "|22.0|\n",
      "|38.0|\n",
      "|26.0|\n",
      "|35.0|\n",
      "|35.0|\n",
      "|30.0|\n",
      "|54.0|\n",
      "| 2.0|\n",
      "|27.0|\n",
      "|14.0|\n",
      "| 4.0|\n",
      "|58.0|\n",
      "|20.0|\n",
      "|39.0|\n",
      "|14.0|\n",
      "|55.0|\n",
      "| 2.0|\n",
      "|30.0|\n",
      "|31.0|\n",
      "|30.0|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined = combined.na.fill(mean_age, subset='Age')\n",
    "combined.select('Age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGsnUz-m_P95"
   },
   "source": [
    "## **Preprocessing Embarked**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHbbamcXMSYP"
   },
   "source": [
    "**Select Embarked, count them, order by count Desc, and save in grouped_Embarked variable:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "v-lRu5vc_FW7"
   },
   "outputs": [],
   "source": [
    "combined_Embarked = combined.select('Embarked').groupBy('Embarked').count().orderBy('count',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1qf5u2IOQrx"
   },
   "source": [
    "**Show groupped_Embarked:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jSFNDTNg_erb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       S|  962|\n",
      "|       C|  253|\n",
      "|       Q|  111|\n",
      "|    null|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_Embarked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzQWYgKBMrbp"
   },
   "source": [
    "**Get the top groupped_Embarked:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "uu46QWrn_gCX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Embarked|\n",
      "+--------+\n",
      "|       S|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_groupped_Embarked = combined_Embarked.select('Embarked').where(col('count') == 962)\n",
    "top_groupped_Embarked.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 116:=============================================>       (173 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|       962|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined_Embarked.agg(max('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8vhoEs8N2w_"
   },
   "source": [
    "**Fill missing values with Top 'S' of grouped_Embarked:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LdzQCRud_mAa"
   },
   "outputs": [],
   "source": [
    "combined = combined.na.fill('S', subset='Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:==========================================>          (161 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       S|  965|\n",
      "|       C|  253|\n",
      "|       Q|  111|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined.select('Embarked').groupBy('Embarked').count().orderBy('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEcdV5Vb_qR_"
   },
   "source": [
    "## **Preprocessing Cabin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BQzPs7tqhpA"
   },
   "source": [
    "**Replace \"cabin\" column with first char from the string:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "4b6L5pK0_nQz"
   },
   "outputs": [],
   "source": [
    "combined = combined.withColumn('Cabin', split(col('Cabin'),\"\").getItem(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H8XshnYj4k2"
   },
   "source": [
    "**Show the result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "gJUQwnG1Oj2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Cabin|\n",
      "+-----+\n",
      "| null|\n",
      "|    C|\n",
      "| null|\n",
      "|    C|\n",
      "| null|\n",
      "| null|\n",
      "|    E|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "|    G|\n",
      "|    C|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.select('Cabin').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzSDsWsUj9Im"
   },
   "source": [
    "**Create the temporary view:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "MR7CXTY7_tMJ"
   },
   "outputs": [],
   "source": [
    "combined.createOrReplaceTempView('combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv7lfQFkrLlN"
   },
   "source": [
    "**Select \"Cabin\" column, count Cabin column, Group by \"Cabin\" column, Order By count DESC**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "A0tZG_mvrKXv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 120:===============================================>     (178 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Cabin|count|\n",
      "+-----+-----+\n",
      "| null| 1021|\n",
      "|    C|   82|\n",
      "|    B|   77|\n",
      "|    D|   52|\n",
      "|    E|   51|\n",
      "|    A|   23|\n",
      "|    F|   18|\n",
      "|    G|    4|\n",
      "|    T|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined.select('Cabin').groupBy('Cabin').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GR6j0LOsB4y"
   },
   "source": [
    "**Fill missing values with \"U\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "mwq5CHEz_up_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Cabin|\n",
      "+-----+\n",
      "|    U|\n",
      "|    C|\n",
      "|    U|\n",
      "|    C|\n",
      "|    U|\n",
      "|    U|\n",
      "|    E|\n",
      "|    U|\n",
      "|    U|\n",
      "|    U|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined = combined.na.fill('U', subset='Cabin')\n",
    "combined.select('Cabin').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRnhA_5-0Hi4"
   },
   "source": [
    "**StringIndexer: A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType. Its default value is ‘frequencyDesc’.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RIKlOX71GQ-"
   },
   "source": [
    "**StringIndexer(inputCol=None, outputCol=None)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0c_Hf_b0R12"
   },
   "source": [
    "**Pipeline: ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWfXaZ0I4dXD"
   },
   "source": [
    "____________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzFCa54R15-g"
   },
   "source": [
    "**Use Pipline to fit and transform:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FsiLsd9452v"
   },
   "source": [
    "**VectorAssembler: VectorAssembler(*, inputCols=None, outputCol=None) A feature transformer that merges multiple columns into a vector column.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU8DeZfh7JIo"
   },
   "source": [
    "**Use randomSplit function and split data to x_train, and X_test with 80% and 20% Consecutive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8C11xf1iAzKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1060 rows in the training set, and 269 in the test set.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = combined.randomSplit([0.8,0.2],seed=0)\n",
    "print(f\"There are {X_train.count()} rows in the training set, and {X_test.count()} in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Title']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalColumns = [col for (col, dtype) in X_train.dtypes\n",
    "                   if dtype == \"string\"]\n",
    "categoricalColumns           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex', 'Ticket', 'Cabin', 'Embarked', 'Title']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalColumns.remove(\"Name\")\n",
    "categoricalColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex_Index', 'Ticket_Index', 'Cabin_Index', 'Embarked_Index', 'Title_Index']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexOutputColumns = [x + \"_Index\" for x in categoricalColumns]\n",
    "indexOutputColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex_OHE', 'Ticket_OHE', 'Cabin_OHE', 'Embarked_OHE', 'Title_OHE']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oheOutputColumns = [x + \"_OHE\" for x in categoricalColumns]\n",
    "oheOutputColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCols=categoricalColumns,\n",
    "                             outputCols=indexOutputColumns,\n",
    "                             handleInvalid='skip')\n",
    "\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputColumns,\n",
    "                          outputCols=oheOutputColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericColumns = [field for (field,dataType) in X_train.dtypes\n",
    "              if ((dataType=='double'or dataType=='int')& (field!='Survived'))]\n",
    "numericColumns.remove('PassengerId')\n",
    "numericColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex_OHE',\n",
       " 'Ticket_OHE',\n",
       " 'Cabin_OHE',\n",
       " 'Embarked_OHE',\n",
       " 'Title_OHE',\n",
       " 'Pclass',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInputs = oheOutputColumns + numericColumns\n",
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJQvmFai72O7"
   },
   "source": [
    "**Build RandomForestClassifier model and use pipeline to fit and transform then display \"prediction, Survived, features\" columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "MzIDSJzgA035"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol='features', labelCol='Survived', predictionCol='prediction', maxDepth=5)\n",
    "pipeline = Pipeline(stages = [stringIndexer,oheEncoder,vecAssembler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/05 07:50:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|            features|Survived|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|(638,[0,119,619,6...|       0|       0.0|\n",
      "|(638,[381,619,628...|       1|       0.0|\n",
      "|(638,[480,619,627...|       0|       0.0|\n",
      "|(638,[309,621,627...|       1|       1.0|\n",
      "|(638,[0,382,619,6...|       1|       0.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF.select('features','Survived','prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSXEI8-r8bKY"
   },
   "source": [
    "**Use MulticlassClassificationEvaluator and set the \"labelCol\" to \"Survived\",  \"predictionCol\" to \"prediction\", \"metricName\" to \"accuracy\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Rl0UAKCaBDO-"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "MCE = MulticlassClassificationEvaluator(predictionCol='prediction',\n",
    "                                        labelCol='Survived',\n",
    "                                        metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7346938775510204"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = MCE.evaluate(predDF)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practical_work.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
